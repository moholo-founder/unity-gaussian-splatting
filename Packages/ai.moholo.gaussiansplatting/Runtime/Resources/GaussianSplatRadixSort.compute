// Radix Sort for Gaussian Splatting
// Based on VkRadixSort by Mirco Werner: https://github.com/MircoWerner/VkRadixSort
// Compatible with OpenGL ES 3.1+ and Vulkan - does NOT use wave intrinsics

#pragma kernel CSCalcDistances
#pragma kernel CSHistogram
#pragma kernel CSGlobalOffsets
#pragma kernel CSScatter
#pragma kernel CSInitIdentity

// No wave intrinsics required!
// #pragma require wavebasic  -- NOT USED
// #pragma require waveballot -- NOT USED

#define WORKGROUP_SIZE 256
#define RADIX_SORT_BINS 256
#define BITS_PER_PASS 8

// ============================================================================
// Buffers
// ============================================================================

// Input/Output buffers for sorting
RWStructuredBuffer<uint> _SortKeys;           // Keys to sort (distances)
RWStructuredBuffer<uint> _SortKeysAlt;        // Alternate buffer for ping-pong
RWStructuredBuffer<uint> _SortPayload;        // Payload (indices)
RWStructuredBuffer<uint> _SortPayloadAlt;     // Alternate payload buffer

// Histogram buffer: [histogram_of_workgroup_0 | histogram_of_workgroup_1 | ... ]
RWStructuredBuffer<uint> _Histograms;

// Global offsets for scattering
RWStructuredBuffer<uint> _GlobalOffsets;

// Gaussian Splatting specific
// For GLES packed format with precomputed covariance: _SplatPosCovA contains (pos.xyz, cov.xx)
// We only need pos.xyz for sorting
StructuredBuffer<float4> _SplatPosCovA;  // xyz=position, w=cov.xx (unused for sorting)
uint _SplatCount;
float4x4 _MatrixMV;
float4x4 _MatrixVP;  // View-Projection matrix for frustum culling

// Frustum culling parameters
float _FrustumCullMargin;  // Extra margin for frustum culling (in NDC space, e.g., 0.1 = 10% margin)
RWStructuredBuffer<uint> _VisibleCount;  // Atomic counter for visible splats

// Sort parameters
uint _NumElements;
uint _RadixShift;
uint _NumWorkgroups;
uint _NumBlocksPerWorkgroup;

// ============================================================================
// Shared Memory
// ============================================================================

groupshared uint gs_Histogram[RADIX_SORT_BINS];
groupshared uint gs_GlobalOffsets[RADIX_SORT_BINS];

struct BinFlags {
    uint flags[WORKGROUP_SIZE / 32];
};
groupshared BinFlags gs_BinFlags[RADIX_SORT_BINS];

// ============================================================================
// Helper Functions
// ============================================================================

// Radix Tricks by Michael Herf - http://stereopsis.com/radix.html
// Convert float to sortable uint (handles negative floats correctly)
uint FloatToSortableUint(float f)
{
    uint fu = asuint(f);
    uint mask = -((int)(fu >> 31)) | 0x80000000;
    return fu ^ mask;
}

// Extract the digit (bin index) for current radix pass
uint ExtractDigit(uint key)
{
    return (key >> _RadixShift) & (RADIX_SORT_BINS - 1);
}

// ============================================================================
// Kernel 4: Initialize Identity Indices (GPU-side initialization)
// ============================================================================
[numthreads(WORKGROUP_SIZE, 1, 1)]
void CSInitIdentity(uint3 id : SV_DispatchThreadID)
{
    uint idx = id.x;
    if (idx >= _NumElements)
        return;
    
    // Initialize payload with identity indices (0, 1, 2, ...)
    _SortPayload[idx] = idx;
}

// ============================================================================
// Kernel 5: Reset Visible Count (call before CSCalcDistances)
// ============================================================================
#pragma kernel CSResetVisibleCount
[numthreads(1, 1, 1)]
void CSResetVisibleCount(uint3 id : SV_DispatchThreadID)
{
    _VisibleCount[0] = 0;
}

// ============================================================================
// Kernel 0: Calculate Distances with Frustum Culling
// ============================================================================
[numthreads(WORKGROUP_SIZE, 1, 1)]
void CSCalcDistances(uint3 id : SV_DispatchThreadID)
{
    uint idx = id.x;
    if (idx >= _SplatCount)
        return;

    // Get original index from payload
    uint origIdx = _SortPayload[idx];
    
    // Transform position to view space (xyz from float4, w is cov.xx which we ignore for sorting)
    float3 pos = _SplatPosCovA[origIdx].xyz;
    float3 viewPos = mul(_MatrixMV, float4(pos, 1.0)).xyz;
    
    // First check: is it behind the camera? (viewPos.z > 0 in Unity view space)
    // This is the most reliable check for near-camera objects
    bool behindCamera = viewPos.z > 0.0;
    
    bool outsideFrustum = behindCamera;
    
    if (!behindCamera)
    {
        // Only do frustum check if object is in front of camera
        // Use clip-space test to avoid division issues when w is small
        float4 clipPos = mul(_MatrixVP, float4(pos, 1.0));
        
        float margin = _FrustumCullMargin;
        // For very close objects, w can be small. Use larger minimum threshold
        // to prevent precision issues that cause flickering near the camera
        float w = max(clipPos.w, 0.1);
        float wMargin = w * (1.0 + margin);
        
        outsideFrustum = (clipPos.x < -wMargin || clipPos.x > wMargin ||
                          clipPos.y < -wMargin || clipPos.y > wMargin);
    }
    
    if (outsideFrustum)
    {
        // Mark culled splats with maximum distance (will be sorted to end)
        _SortKeys[idx] = 0xFFFFFFFF;
    }
    else
    {
        // Visible splat: count it and compute normal distance
        InterlockedAdd(_VisibleCount[0], 1);
        
        // Convert view-space Z to sortable uint for back-to-front rendering
        // In Unity view space, objects in front have NEGATIVE Z (camera looks down -Z)
        // For back-to-front: far objects (more negative Z) should be drawn FIRST (lower indices)
        // Radix sort is ascending, so we want far objects to have SMALLER keys
        _SortKeys[idx] = FloatToSortableUint(viewPos.z);
    }
}

// ============================================================================
// Kernel 1: Build Histograms (per workgroup)
// ============================================================================
[numthreads(WORKGROUP_SIZE, 1, 1)]
void CSHistogram(uint3 gtid : SV_GroupThreadID, uint3 gid : SV_GroupID)
{
    uint lID = gtid.x;
    uint wID = gid.x;
    
    // Initialize local histogram
    if (lID < RADIX_SORT_BINS)
    {
        gs_Histogram[lID] = 0;
    }
    GroupMemoryBarrierWithGroupSync();
    
    // Count elements in each bin
    for (uint index = 0; index < _NumBlocksPerWorkgroup; index++)
    {
        uint elementId = wID * _NumBlocksPerWorkgroup * WORKGROUP_SIZE + index * WORKGROUP_SIZE + lID;
        if (elementId < _NumElements)
        {
            uint bin = ExtractDigit(_SortKeys[elementId]);
            InterlockedAdd(gs_Histogram[bin], 1);
        }
    }
    GroupMemoryBarrierWithGroupSync();
    
    // Write histogram to global memory
    if (lID < RADIX_SORT_BINS)
    {
        _Histograms[RADIX_SORT_BINS * wID + lID] = gs_Histogram[lID];
    }
}

// ============================================================================
// Kernel 2: Compute Global Offsets (prefix sum across all workgroups)
// ============================================================================
[numthreads(WORKGROUP_SIZE, 1, 1)]
void CSGlobalOffsets(uint3 gtid : SV_GroupThreadID, uint3 gid : SV_GroupID)
{
    uint lID = gtid.x;
    uint wID = gid.x;
    
    // Each thread handles one bin
    if (lID < RADIX_SORT_BINS)
    {
        // Compute prefix sum across all workgroups for this bin
        uint localOffset = 0;
        uint totalCount = 0;
        
        for (uint j = 0; j < _NumWorkgroups; j++)
        {
            uint histValue = _Histograms[RADIX_SORT_BINS * j + lID];
            
            // Store the local offset for our workgroup
            if (j == wID)
            {
                localOffset = totalCount;
            }
            totalCount += histValue;
        }
        
        // Store in shared memory for prefix sum
        gs_Histogram[lID] = totalCount;
    }
    GroupMemoryBarrierWithGroupSync();
    
    // Compute exclusive prefix sum over all bins using shared memory
    // This is a simple sequential prefix sum - could be optimized with parallel scan
    if (lID == 0)
    {
        uint sum = 0;
        for (uint i = 0; i < RADIX_SORT_BINS; i++)
        {
            uint val = gs_Histogram[i];
            gs_GlobalOffsets[i] = sum;
            sum += val;
        }
    }
    GroupMemoryBarrierWithGroupSync();
    
    // Each workgroup computes its own starting offset
    if (lID < RADIX_SORT_BINS)
    {
        // Recompute local offset for this workgroup
        uint localOffset = 0;
        for (uint j = 0; j < wID; j++)
        {
            localOffset += _Histograms[RADIX_SORT_BINS * j + lID];
        }
        
        // Global offset = prefix sum of all bins before this one + local offset within this bin
        _GlobalOffsets[RADIX_SORT_BINS * wID + lID] = gs_GlobalOffsets[lID] + localOffset;
    }
}

// ============================================================================
// Kernel 3: Scatter (reorder elements based on computed offsets)
// ============================================================================
[numthreads(WORKGROUP_SIZE, 1, 1)]
void CSScatter(uint3 gtid : SV_GroupThreadID, uint3 gid : SV_GroupID)
{
    uint lID = gtid.x;
    uint wID = gid.x;
    
    // Load global offsets into shared memory
    if (lID < RADIX_SORT_BINS)
    {
        gs_GlobalOffsets[lID] = _GlobalOffsets[RADIX_SORT_BINS * wID + lID];
    }
    
    uint flagsBin = lID / 32;
    uint flagsBit = 1u << (lID % 32);
    
    // Process blocks
    for (uint index = 0; index < _NumBlocksPerWorkgroup; index++)
    {
        uint elementId = wID * _NumBlocksPerWorkgroup * WORKGROUP_SIZE + index * WORKGROUP_SIZE + lID;
        
        // Initialize bin flags
        if (lID < RADIX_SORT_BINS)
        {
            [unroll]
            for (uint i = 0; i < WORKGROUP_SIZE / 32; i++)
            {
                gs_BinFlags[lID].flags[i] = 0;
            }
        }
        GroupMemoryBarrierWithGroupSync();
        
        uint element = 0;
        uint payload = 0;
        uint binID = 0;
        uint binOffset = 0;
        
        if (elementId < _NumElements)
        {
            element = _SortKeys[elementId];
            payload = _SortPayload[elementId];
            binID = ExtractDigit(element);
            binOffset = gs_GlobalOffsets[binID];
            
            // Mark our presence in this bin using OR (each thread sets its bit)
            InterlockedOr(gs_BinFlags[binID].flags[flagsBin], flagsBit);
        }
        GroupMemoryBarrierWithGroupSync();
        
        if (elementId < _NumElements)
        {
            // Calculate output index within this bin
            uint prefix = 0;
            uint count = 0;
            
            [unroll]
            for (uint i = 0; i < WORKGROUP_SIZE / 32; i++)
            {
                uint bits = gs_BinFlags[binID].flags[i];
                uint fullCount = countbits(bits);
                uint partialCount = countbits(bits & (flagsBit - 1));
                
                prefix += (i < flagsBin) ? fullCount : 0;
                prefix += (i == flagsBin) ? partialCount : 0;
                count += fullCount;
            }
            
            // Write to output
            uint outputIdx = binOffset + prefix;
            _SortKeysAlt[outputIdx] = element;
            _SortPayloadAlt[outputIdx] = payload;
            
            // Last thread in this bin updates the global offset
            if (prefix == count - 1)
            {
                InterlockedAdd(gs_GlobalOffsets[binID], count);
            }
        }
        
        GroupMemoryBarrierWithGroupSync();
    }
}

